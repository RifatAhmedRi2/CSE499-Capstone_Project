{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJTF7DZf-hVz"
      },
      "outputs": [],
      "source": [
        "!pip install gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwfrcJwQAd9F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] accelerate evaluate datasets peft gdown -q\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-8Z0ha6Rqr1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1Va7X8-rZT"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/file/d/135goeAyfKzasCtME-gZoEY6A9f9a9Xm5/view?usp=sharing\", output=\"train.rar\", quiet=False, fuzzy=True)\n",
        "gdown.download(\"https://drive.google.com/file/d/1W4EdUSKG-txm0wY2gTh6wFgVXX2iK6b7/view?usp=sharing\", output=\"test.rar\", quiet=False, fuzzy=True)\n",
        "gdown.download(\"https://drive.google.com/file/d/1aXnuT8I3QVkJcnqV4RL3gtgsgRr-ZrKC/view?usp=sharing\", output=\"hold.rar\", quiet=False, fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcSRDaaVYYJy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wixTRd2cYON2"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEbCDkZf-yJb"
      },
      "outputs": [],
      "source": [
        "!unrar x 'train.rar' 'train'\n",
        "!unrar x 'test.rar' 'test'\n",
        "!unrar x 'hold.rar' 'hold'\n",
        "# !rm *.rar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN3sDzQj-f5u"
      },
      "source": [
        "## Building Damage Image Masking: Image Segmentation with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkIZ58J-f5y"
      },
      "source": [
        "#### Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JI3YnkJ-f5z"
      },
      "outputs": [],
      "source": [
        "import os, sys, glob, shutil\n",
        "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import evaluate\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm.contrib.concurrent import process_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZhRMQNv-f53"
      },
      "source": [
        "#### Display Library Versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm-c5fwl-f54"
      },
      "outputs": [],
      "source": [
        "print(\"Python :\".rjust(18), sys.version[0:6])\n",
        "print(\"NumPy :\".rjust(18), np.__version__)\n",
        "print(\"Pandas :\".rjust(18), pd.__version__)\n",
        "print(\"Torch :\".rjust(18), torch.__version__)\n",
        "print(\"Torch Vision :\".rjust(18), torchvision.__version__)\n",
        "print(\"Transformers :\".rjust(18), transformers.__version__)\n",
        "print(\"Evaluate :\".rjust(18), evaluate.__version__)\n",
        "print(\"PEFT :\".rjust(18), peft.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy047WMP-f56"
      },
      "source": [
        "#### Basic Values/Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuimvMuY-f57"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"nvidia/mit-b0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvwW60oC-f57"
      },
      "source": [
        "#### Create Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP2tUMV_ISGD"
      },
      "outputs": [],
      "source": [
        "def img_rewrite(image_path):\n",
        "  image = cv2.imread(image_path, 0)\n",
        "  image[image>0] = 1\n",
        "  cv2.imwrite(image_path, image)\n",
        "\n",
        "images = glob.glob(\"/content/*/targets/*\")\n",
        "\n",
        "process_map(\n",
        "        img_rewrite,\n",
        "        images\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLVKERLC-f58"
      },
      "outputs": [],
      "source": [
        "class ImageSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class to create an Image\n",
        "    (Semantic) Segmentation dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parent_dir, sub_path, image_processor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            parent_dir (string): Root directory of the dataset containing the train, dev, test.\n",
        "            sub_path (string): sub directory containing images + annotations\n",
        "            image_processor: image processor used to prepare images + segmentation maps.\n",
        "        \"\"\"\n",
        "        self.parent_dir = parent_dir\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "        self.img_dir = os.path.join(self.parent_dir, sub_path, \"images\")\n",
        "        self.ann_dir = os.path.join(self.parent_dir, sub_path, \"targets\")\n",
        "\n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "            image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "\n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "            annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \\\n",
        "            \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
        "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "\n",
        "        # randomly crop + pad both image and segmentation map to same size\n",
        "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "            encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oul-a0Ga-f5-"
      },
      "source": [
        "#### Define Image Processor & Ingest Training & Testing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOVVnPoG-f5-"
      },
      "outputs": [],
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=False)\n",
        "\n",
        "data_folder = \"/content/data\"\n",
        "\n",
        "train_ds = ImageSegmentationDataset(parent_dir=data_folder, sub_path=\"train\",\n",
        "                                      image_processor=image_processor)\n",
        "test_ds = ImageSegmentationDataset(parent_dir=data_folder, sub_path=\"test\",\n",
        "                                      image_processor=image_processor,)\n",
        "hold_ds = ImageSegmentationDataset(parent_dir=data_folder, sub_path=\"hold\",\n",
        "                                      image_processor=image_processor,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzjEul3G-f6A"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(train_ds))\n",
        "print(\"Number of validation examples:\", len(test_ds))\n",
        "print(\"Number of validation examples:\", len(hold_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEAErUw2-f6A"
      },
      "source": [
        "#### Shape of Sample (Pixel Values Feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxjjJcez-f6A"
      },
      "outputs": [],
      "source": [
        "sample = train_ds[12]\n",
        "\n",
        "sample['pixel_values'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkwcdJQA-f6B"
      },
      "source": [
        "#### Shape of Sample (Labels Feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh3hKmwZ-f6B"
      },
      "outputs": [],
      "source": [
        "sample['labels'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJzgXnzc-f6C"
      },
      "source": [
        "#### Sample Tensors Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiUB4e9D-f6C"
      },
      "outputs": [],
      "source": [
        "sample['pixel_values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcqf-swjKJUY"
      },
      "outputs": [],
      "source": [
        "sample['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQtZC5hF-f6D"
      },
      "source": [
        "#### Unique Label Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_Ww5wAH-f6D"
      },
      "outputs": [],
      "source": [
        "sample['labels'].squeeze().unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TMrVF38-f6D"
      },
      "source": [
        "#### Create Conversions Between String & Integer Values For Label Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNvVXAVa-f6E"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"background\", 1: \"building\"}\n",
        "label2id = {label: idx for idx, label in id2label.items()}\n",
        "\n",
        "num_labels = len(id2label)\n",
        "\n",
        "print(f\"List of Unique Label Values: {id2label}\")\n",
        "print(f\"Number of Unique Label Values: {num_labels}\")\n",
        "print(f\"label2id: {label2id}\")\n",
        "print(f\"id2label: {id2label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2fYM-Zn-f6E"
      },
      "source": [
        "#### Define Compute Metrics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7_5uTCz-f6E"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"mean_iou\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    with torch.no_grad():\n",
        "        logits, labels = eval_pred\n",
        "        logits_tensor = torch.from_numpy(logits)\n",
        "        logits_tensor = nn.functional.interpolate(\n",
        "            logits_tensor,\n",
        "            size=labels.shape[-2:],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        ).argmax(dim=1)\n",
        "\n",
        "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
        "        # currently using _compute instead of compute\n",
        "        # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
        "        metrics = metric._compute(\n",
        "            predictions=pred_labels,\n",
        "            references=labels,\n",
        "            num_labels=len(id2label),\n",
        "            ignore_index=0,\n",
        "            reduce_labels=image_processor.do_reduce_labels,\n",
        "        )\n",
        "\n",
        "        per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "        per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "        metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "        metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et6vaFyX-f6F"
      },
      "source": [
        "#### Define Function to Display Number of Trainable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENdQ9v80-f6F"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL1HcOk2-f6F"
      },
      "source": [
        "#### Define Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIndMhcC-f6G"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
        "    checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
        ")\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU8eG-Ft-f6G"
      },
      "source": [
        "#### Define LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl0ht7SQ-f6G"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"lora_only\",\n",
        "    modules_to_save=[\"decode_head\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKjzJm0R-f6H"
      },
      "source": [
        "#### Wrap Base Model With LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xktlnsva-f6H"
      },
      "outputs": [],
      "source": [
        "lora_model = get_peft_model(model, config)\n",
        "\n",
        "print_trainable_parameters(lora_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykzG-wVz-f6I"
      },
      "source": [
        "#### Define Early Stopping Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Wja8C3-f6I"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = [EarlyStoppingCallback(early_stopping_patience=4)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI7Ciu0O-f6J"
      },
      "source": [
        "#### Define Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DgmgZZ5-f6J"
      },
      "outputs": [],
      "source": [
        "model_name = checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-building-damage-lora\",\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=25,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_total_limit=3,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=5,\n",
        "    remove_unused_columns=False,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    label_names=[\"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBB9f3xR-f6J"
      },
      "source": [
        "#### Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DID8FZ_-f6J"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=early_stopping_callback,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mbqjk7--f6J"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJb0gBZ8-f6J"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vlh-Lfy-f6K"
      },
      "source": [
        "#### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I_JKJFm-f6K"
      },
      "outputs": [],
      "source": [
        "model_id = \"segformer-building-damage-lora\"\n",
        "lora_model.save_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45aY-uW-f6L"
      },
      "source": [
        "#### Push Model to Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak-_SLHF-f6L"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(f\"Full data model!!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_t8Y4Rq_FVC"
      },
      "source": [
        "# Let’s now prepare an inference_model and run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyHVSqap_Cfx"
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_id)\n",
        "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
        "    checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(model, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc45YiEk_LV7"
      },
      "source": [
        "Get an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffLprwUN_MQm"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"/content/data/test/images/socal-fire_00001387_post_disaster.png\")\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAzoQ-OP_Omu"
      },
      "source": [
        "Preprocess the image to prepare for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGW14dOq_S5S"
      },
      "outputs": [],
      "source": [
        "encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8o-7Us_XJi"
      },
      "source": [
        "Run inference with the encoded image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfr-2XXN_Zc2"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = inference_model(pixel_values=encoding.pixel_values)\n",
        "    logits = outputs.logits\n",
        "\n",
        "upsampled_logits = nn.functional.interpolate(\n",
        "    logits,\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bilinear\",\n",
        "    align_corners=False,\n",
        ")\n",
        "\n",
        "pred_seg = upsampled_logits.argmax(dim=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Z9PhDyTDWQ"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.imshow(image)\n",
        "ax1.set_title('Image')\n",
        "ax2.imshow(pred_seg)\n",
        "ax2.set_title('Segmentation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUKzHAou_d0i"
      },
      "source": [
        "\n",
        "\n",
        "Next, visualize the results. We need a color palette for this. Here, we use ade_palette(). As it is a long array, so we don’t include it in this guide, please copy it from the TensorFlow Model Garden repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x305qWa_fao"
      },
      "outputs": [],
      "source": [
        "def create_ade20k_label_colormap():\n",
        "  \"\"\"Creates a label colormap used in ADE20K segmentation benchmark.\n",
        "\n",
        "  Returns:\n",
        "    A colormap for visualizing segmentation results.\n",
        "  \"\"\"\n",
        "  return np.asarray([[0, 0, 0],[120, 120, 120],[180, 120, 120],[6, 230, 230],[80, 50, 50],[4, 200, 3],[120, 120, 80],[140, 140, 140],[204, 5, 255],\n",
        "    [230, 230, 230],[4, 250, 7],[224, 5, 255],[235, 255, 7],[150, 5, 61],[120, 120, 70],[8, 255, 51],[255, 6, 82],[143, 255, 140],[204, 255, 4],\n",
        "    [255, 51, 7],[204, 70, 3],[0, 102, 200],[61, 230, 250],[255, 6, 51],[11, 102, 255],[255, 7, 71],[255, 9, 224],[9, 7, 230],[220, 220, 220],\n",
        "    [255, 9, 92],[112, 9, 255],[8, 255, 214],[7, 255, 224],[255, 184, 6],[10, 255, 71],[255, 41, 10],[7, 255, 255],[224, 255, 8],[102, 8, 255],\n",
        "    [255, 61, 6],[255, 194, 7],[255, 122, 8],[0, 255, 20],[255, 8, 41],[255, 5, 153],[6, 51, 255],[235, 12, 255],[160, 150, 20],[0, 163, 255],\n",
        "    [140, 140, 140],[250, 10, 15],[20, 255, 0],[31, 255, 0],[255, 31, 0],[255, 224, 0],[153, 255, 0],[0, 0, 255],[255, 71, 0],[0, 235, 255],\n",
        "    [0, 173, 255],[31, 0, 255],[11, 200, 200],[255, 82, 0],[0, 255, 245],[0, 61, 255],[0, 255, 112],[0, 255, 133],[255, 0, 0],[255, 163, 0],\n",
        "    [255, 102, 0],[194, 255, 0],[0, 143, 255],[51, 255, 0],[0, 82, 255],[0, 255, 41],[0, 255, 173],[10, 0, 255],[173, 255, 0],[0, 255, 153],\n",
        "    [255, 92, 0],[255, 0, 255],[255, 0, 245],[255, 0, 102],[255, 173, 0],[255, 0, 20],[255, 184, 184],[0, 31, 255],[0, 255, 61],[0, 71, 255],\n",
        "    [255, 0, 204],[0, 255, 194],[0, 255, 82],[0, 10, 255],[0, 112, 255],[51, 0, 255],[0, 194, 255],[0, 122, 255],[0, 255, 163],[255, 153, 0],\n",
        "    [0, 255, 10],[255, 112, 0],[143, 255, 0],[82, 0, 255],[163, 255, 0],[255, 235, 0],[8, 184, 170],[133, 0, 255],[0, 255, 92],[184, 0, 255],\n",
        "    [255, 0, 31],[0, 184, 255],[0, 214, 255],[255, 0, 112],[92, 255, 0],[0, 224, 255],[112, 224, 255],[70, 184, 160],[163, 0, 255],[153, 0, 255],\n",
        "    [71, 255, 0],[255, 0, 163],[255, 204, 0],[255, 0, 143],[0, 255, 235],[133, 255, 0],[255, 0, 235],[245, 0, 255],[255, 0, 122],[255, 245, 0],\n",
        "    [10, 190, 212],[214, 255, 0],[0, 204, 255],[20, 0, 255],[255, 255, 0],[0, 153, 255],[0, 41, 255],[0, 255, 204],[41, 0, 255],[41, 255, 0],\n",
        "    [173, 0, 255],[0, 245, 255],[71, 0, 255],[122, 0, 255],[0, 255, 184],[0, 92, 255],[184, 255, 0],[0, 133, 255],[255, 214, 0],[25, 194, 194],\n",
        "    [102, 255, 0],[92, 0, 255],\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnnlJr9S_hV8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
        "palette = np.array(create_ade20k_label_colormap())\n",
        "\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[pred_seg == label, :] = color\n",
        "color_seg = color_seg[..., ::-1]  # convert to BGR\n",
        "\n",
        "img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
        "img = img.astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
